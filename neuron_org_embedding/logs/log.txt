I20231217 21:31:10 169998 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:31:10 169998 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:31:10 169998 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:31:10 169998 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:31:10 169998 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:31:11 169998 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:31:11 169998 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:32:09 170370 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:32:09 170370 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:32:09 170370 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:32:09 170370 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:32:10 170370 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:32:11 170370 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:32:11 170370 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:33:38 170741 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:33:38 170741 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:33:38 170741 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:33:38 170741 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:33:39 170741 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:33:40 170741 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:33:40 170741 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:34:48 171099 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:34:48 171099 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:34:48 171099 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:34:48 171099 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:34:49 171099 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:34:50 171099 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:34:50 171099 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:35:32 171461 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:35:32 171461 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:35:32 171461 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:35:32 171461 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:35:33 171461 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:35:34 171461 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:35:34 171461 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:36:08 171818 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:36:08 171818 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:36:08 171818 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:36:08 171818 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:36:08 171818 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:36:09 171818 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:36:09 171818 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:36:40 172185 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:36:40 172185 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:36:41 172185 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:36:41 172185 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:36:41 172185 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:36:42 172185 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:36:42 172185 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:37:16 172542 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:37:16 172542 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:37:16 172542 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:37:16 172542 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:37:16 172542 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:37:17 172542 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:37:17 172542 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:37:52 172908 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:37:52 172908 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:37:52 172908 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:37:52 172908 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:37:52 172908 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:37:53 172908 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:37:53 172908 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:43:01 173377 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:43:01 173377 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:43:01 173377 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:43:01 173377 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:43:01 173377 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:43:02 173377 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:43:02 173377 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:43:32 173740 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:43:32 173740 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:43:32 173740 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:43:32 173740 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:43:33 173740 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:43:34 173740 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:43:34 173740 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:44:34 174102 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:44:34 174102 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:44:34 174102 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:44:34 174102 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:44:34 174102 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:44:35 174102 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:44:35 174102 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:44:35 174102 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:44:35 174102 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:44:35 174102 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:44:35 174102 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:44:36 174102 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:44:36 174102 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:45:40 174469 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:45:40 174469 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:45:40 174469 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:45:41 174469 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:45:41 174469 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:45:42 174469 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:45:42 174469 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:50:00 174897 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:50:00 174897 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:50:00 174897 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:50:00 174897 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:50:00 174897 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:50:01 174897 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:50:01 174897 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:52:53 175319 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:52:53 175319 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:52:53 175319 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:52:53 175319 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:52:53 175319 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:52:54 175319 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:52:54 175319 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:55:01 175741 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:55:01 175741 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:55:01 175741 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:55:01 175741 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:55:02 175741 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:55:03 175741 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:55:03 175741 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:57:42 176163 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:57:42 176163 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:57:42 176163 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:57:42 176163 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:57:43 176163 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:57:44 176163 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:57:44 176163 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 21:59:35 176590 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 21:59:35 176590 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 21:59:35 176590 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 21:59:35 176590 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 21:59:35 176590 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 21:59:36 176590 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 21:59:37 176590 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 22:02:06 177073 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 22:02:06 177073 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 22:02:06 177073 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 22:02:06 177073 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 22:02:07 177073 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 22:02:08 177073 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 22:02:08 177073 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 22:14:45 177943 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 22:14:45 177943 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 22:14:45 177943 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 22:14:45 177943 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 22:14:45 177943 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 22:14:46 177943 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 22:14:46 177943 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 23:26:00 189897 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 23:26:00 189897 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 23:26:00 189897 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 23:26:00 189897 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 23:26:01 189897 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 23:26:02 189897 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 23:26:02 189897 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 23:27:07 190587 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 23:27:07 190587 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 23:27:07 190587 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 23:27:07 190587 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 23:27:07 190587 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 23:27:08 190587 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 23:27:08 190587 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 23:28:15 191120 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 23:28:15 191120 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 23:28:15 191120 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 23:28:15 191120 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 23:28:16 191120 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 23:28:17 191120 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 23:28:17 191120 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 23:34:24 192363 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 23:34:24 192363 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 23:34:24 192363 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 23:34:24 192363 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 23:34:24 192363 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 23:34:25 192363 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 23:34:25 192363 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231217 23:35:19 192895 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231217 23:35:19 192895 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231217 23:35:19 192895 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231217 23:35:19 192895 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231217 23:35:20 192895 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231217 23:35:21 192895 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231217 23:35:21 192895 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231218 13:49:22 211316 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231218 13:49:22 211316 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231218 13:49:22 211316 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231218 13:49:22 211316 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231218 13:49:22 211316 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231218 13:49:23 211316 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231218 13:49:23 211316 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231218 14:05:11 213346 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231218 14:05:11 213346 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231218 14:05:11 213346 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231218 14:05:11 213346 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231218 14:05:11 213346 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231218 14:05:12 213346 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231218 14:05:12 213346 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20231218 15:12:02 221069 morphFM config.py:59] git:
  sha: 55a848a3b3822fe4d7346f284c9aa06f2d22d84d, status: has uncommitted changes, branch: main

I20231218 15:12:02 221069 morphFM config.py:60] config_file: /mnt/data/aim/liyaxuan/projects/git_project2/configs/ours_final.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding']
output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
I20231218 15:12:02 221069 morphFM config.py:26] sqrt scaling learning rate; base: 0.003, new: 0.00028125000000000003
I20231218 15:12:02 221069 morphFM config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
  koleo_loss_weight: 0.1
  crop_local: false
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  noise_replace_p: 0.1
  separate_head: false
  head_n_prototypes: 16384
  head_bottleneck_dim: 128
  head_nlayers: 3
  head_hidden_dim: 512
train:
  batch_size_per_gpu: 9
  dataset_path: ImageNet:split=TRAIN
  output_dir: /mnt/data/aim/liyaxuan/projects/git_project2/neuron_org_embedding
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1294
  cache_dataset: true
  centering: centering
student:
  arch: vit_large
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
teacher:
  momentum_teacher: 0.992
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 7
optim:
  epochs: 30
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.003
  lr: 0.00028125000000000003
  warmup_epochs: 2
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 4
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 1000
  local_crops_size: 200
evaluation:
  eval_period_iterations: 1294
dataset:
  node_num: 1000
  path: /mnt/data/aim/liyaxuan/pre_data/
  jitter_var: 1
  translate_var: 10
  rotation_axis: 'y'
  n_drop_branch: 10
model:
  dim: 128
  depth: 7
  n_head: 11
  feat_dim: 11
  pos_dim: 128
  num_classes: 1000

I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 128
I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:58] OPTIONS -- DINO
I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 16384
I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 128
I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 512
I20231218 15:12:02 221069 morphFM ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:85] OPTIONS -- IBOT
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_large network.
I20231218 15:12:03 221069 morphFM ssl_meta_arch.py:440] DISTRIBUTED FSDP -- preparing model for distributed training
W20231218 15:12:03 221069 py.warnings warnings.py:109] /root/.local/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

